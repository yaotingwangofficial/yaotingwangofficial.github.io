<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Yaoting Wang - Home Page</title>
    <link rel="stylesheet" href="./css/styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">


    <style>
        .photo-and-contact {
            display: flex; /* 使用 flexbox 布局 */
            align-items: center; /* 垂直居中对齐 */
        }
        .photo-and-contact img {
            margin-right: 15px; /* 设置图片和文本之间的间距 */
        }
        .photo-and-contact p {
            margin: 0; /* 去掉段落的默认外边距 */
        }
    </style>
    <style>
        .intro-list li {
            margin-bottom: 0.5em;
        }
        .news_list li {
            margin-bottom: 0.5em;
        }
    </style>

    <style>
        .custom-size {
            font-size: 24px; /* 你可以根据需要调整大小 */
        }
        .icon {
            text-decoration: none;
        }
    </style>

</head>
<body>
    <div class="container">
        <header>
            <nav>
                <ul>
                    <li><a href="index.html" style="color: cornflowerblue;">Home</a></li>
                    <li><a href="https://github.com/yaotingwangofficial/yaotingwangofficial.github.io/raw/main/yaoting_wang_cv.pdf">Curriculum Vitae</a></li>
                    <li><a href="project.html">Project & Publication</a></li>
                    <li><a href="gallery.html">Gallery</a></li>
                </ul>
            </nav>
            <br>
            <h1>Welcome to Yaoting WANG's Home Page!</h1>
        </header>
        
        <main>
            <!-- <section id="about">
                <h2>关于我</h2>
                <p>你好，我是 [你的名字]。我是一名 [你的职业]，热衷于 [你的兴趣]。我致力于 [你的目标或成就]。</p>
            </section> -->

            <h2>Yaoting WANG</h2>
            <div class="photo-and-contact">
                <img src="media/yaoting.jpg" alt="My Photo" width="18%">
                <div>
                    <p> Yaoting WANG | 王耀霆 </p>
                    <br>
                    <p> Email: yaoting.wang@outlook.com </p>
                    <br>
                    <a href="https://scholar.google.com/citations?user=apa4kr4AAAAJ" target="_blank" class="icon">
                        <img src="./media/google.png" width="15%">
                    </a>
                    
                    <a href="https://github.com/yaotingwangofficial" target="_blank" class="icon">
                        <img src="./media/github.png" width="15%">
                    </a>

                    <a href="https://www.zhihu.com/people/wang-yao-ting-29" target="_blank" class="icon">
                        <img src="./media/zhihu.png" width="15%">
                    </a>
                </div>
            </div>
            
            <div id="intro">
                <h2>Introduction</h2>
                <p> Hallo! I'm Yaoting WANG, and I'm currently a research intern at AIR, Tsinghua University. My primary interests lie in Multimodal LLM, Audio-Visual Intelligence and Segmentation.
                </p>
            </div>

            <div>
                <section id="edu">
                    <h2>Education</h2>
                    <ul class="intro-list" style="list-style-type: none; padding-left: 0;">
                        <li>
                            <strong>Ph.D.</strong> @ Fudan University <span style="color: gray;">(2025 - 2029, expected)</span>
                        </li>
                        <li>
                            <strong>M.Sc.</strong> @ University of Edinburgh <span style="color: gray;">(2021 - 2022)</span>
                        </li>
                        <li>
                            <strong>B.Sc.</strong> @ University of Limerick <span style="color: gray;">(2019 - 2021)</span>
                        </li>
                        <li>
                            <strong>B.Eng.</strong> @ Shandong University of Science and Technology <span style="color: gray;">(2017 - 2021)</span>
                        </li>
                    </ul>
                </section>

                <section id="edu">
                    <h2>Professional</h2>
                    <ul class="intro-list" style="list-style-type: none; padding-left: 0;">
                        <li>
                            <strong>Program Chair</strong> @ MUCG, MM'25 <span style="color: gray;">(07/2025 - 10/2025)</span><br>
                            <span style="margin-left: 2em;">
                                Homepage: <a href="https://mllm-mucg.github.io/MM2025/" target="_blank">MUCG@MM'25</a>.
                            </span>
                        </li>
                        <li>
                            <strong>Research Intern</strong> @ AIR, THU <span style="color: gray;">(02/2025 - 09/2025)</span><br>
                            <span style="margin-left: 2em;">
                                Advised by 
                                <a href="https://scholar.google.com/citations?user=TFGBA9cAAAAJ" target="_blank">Prof. Yunxin Liu</a>.
                            </span>
                        </li>
                        <li>
                            <strong>Visiting Student</strong> @ MiniGPT, KAUST <span style="color: gray;">(04/2024 - 02/2025)</span><br>
                            <span style="margin-left: 2em;">
                                Advised by 
                                <a href="https://scholar.google.com.sg/citations?user=5HM8wcgAAAAJ" target="_blank">Dr. Jian Ding</a> and 
                                <a href="https://scholar.google.com/citations?user=iRBUTOAAAAAJ" target="_blank">Prof. Mohamed Elhoseiny</a>.
                            </span>
                        </li>
                        <li>
                            <strong>Research Assistant</strong> @ GSAI, RUC <span style="color: gray;">(03/2023 - 03/2024)</span><br>
                            <span style="margin-left: 2em;">
                                Advised by 
                                <a href="https://scholar.google.com.hk/citations?user=F7bvTOEAAAAJ" target="_blank">Prof. Di Hu</a>.
                            </span>
                        </li>
                    </ul>
                </section>
            </div>
            
            <section id="news">
                <h2>News</h2>
                <div class="news_list">
                    <ul>
                        <li>
                            [01-07-2025] We hosted the 1st International Workshop on MLLM for Unified Comprehension and Generation (MUCG@MM'25), and I served as the Program Chair. Welcome <a href="https://mllm-mucg.github.io/MM2025/" target="_blank">paper submissions</a>.
                        </li>
                        <li>
                            [29-06-2025] Our paper <a href="https://arxiv.org/abs/2501.02135" target="_blank">"AVTrustBench: Assessing and Enhancing Reliability and Robustness in Audio-Visual LLMs"</a> has been accepted at ICCV 2025.
                        </li>
                        <li>
                            [23-05-2025] We organize the 1st International Workshop on <a href="https://mllm-mucg.github.io/MM2025/" target="_blank">MLLM for Unified Comprehension and Generation</a> (MUCG) at ACM 2025, <a href="https://openreview.net/group?id=acmmm.org/ACMMM/2025/Workshop/MUCG" target="_blank">Call for Papers</a> is now open!
                        </li>
                        <li>
                            [01-05-2025] Our paper <a href="https://generalist.top/" target="_blank">"On Path to Multimodal Generalist"</a> has been accepted as a <b style="color: red;">Spotlight/Oral</b> presentation at ICML 2025!
                        </li>

                        <li>
                            [18-03-2025] We release the first comprehensive survey on <b>multimodal chain-of-thought reasoning</b>, along with the <a href='https://github.com/yaotingwangofficial/Awesome-MCoT' target="_blank">Awesome-MCoT</a> repository.
                        </li>
                        <li> 
                            [30-08-2024] <br> - Thanks <a href="https://www.qbitai.com/2024/08/186120.html" target="_blank">
                            QBitAI's report</a> for our work <a href="https://gewu-lab.github.io/Ref-AVS/" target="_blank">
                            Ref-AVS</a>, welcome to follow!
                        </li>
                        <li> 
                            [09-07-2024] <br> - Glad to have a 
                            <a href="https://www.bilibili.com/video/BV1dT421r7tm/" target="_blank">speech</a> 
                            on GAVS at <a href="https://valser.org/" target="_blank">Vision and Learning SEminar (VASLE)</a>!
                        </li>

                        <li>
                            [16-07-2024] <br> - We are excieted to release our new task 
                            <a href="https://gewu-lab.github.io/Ref-AVS/" target="_blank">Reference Audio-Visual Segmentation (Ref-AVS)</a>
                             and its benchmark dataset Ref-AVS Bench. How can we ask machines to locate objects of interest 
                             in the real-world with vision, audio, language... just like a human!
                        </li>

                        <li> 
                            [01-07-2024] <br> - <span style="color: red;">Three papers</span>
                            <a href="https://arxiv.org/abs/2407.10957" target="_blank">Ref-AVS</a>,
                            <a href="https://arxiv.org/abs/2407.10947" target="_blank">Segmentation-Preference</a> and
                            <a href="https://arxiv.org/abs/2407.11820" target="_blank">Stepping-Stones</a> 
                            have been accepted by <b>ECCV 2024</b>! 
                        </li>
                        <li>
                            [01-03-2024] <br> - Joined MiniGPT Group, Vision-CAIR, KAUST!
                        </li>
                        <li> 
                            [09-12-2023] <br> - One paper 
                            <a href="https://arxiv.org/abs/2309.07929" target="_blank">GAVS</a> 
                            has been accepted by <b>AAAI 2024</b> main track and ICCV 2023 AV4D workshop! 
                        </li>
                        <li> 
                            [05-01-2023] <br> - Joined GeWu Lab, Gaoling School of Artificial Intelligence, Renmin University of China!
                        </li>
                    </ul>
                </div>
            </section>
            
        </main>
        
        <!-- <footer>
            <p>&copy; 2024 [你的名字]. 版权所有.</p>
        </footer> -->
    </div>
</body>
</html>
