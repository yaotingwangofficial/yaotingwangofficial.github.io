<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Yaoting Wang - Home Page</title>
    <link rel="stylesheet" href="./css/styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">


    <style>
        .photo-and-contact {
            display: flex; /* 使用 flexbox 布局 */
            align-items: center; /* 垂直居中对齐 */
        }
        .photo-and-contact img {
            margin-right: 15px; /* 设置图片和文本之间的间距 */
        }
        .photo-and-contact p {
            margin: 0; /* 去掉段落的默认外边距 */
        }
    </style>

    <style>
        .custom-size {
            font-size: 24px; /* 你可以根据需要调整大小 */
        }
        .icon {
            text-decoration: none;
        }
 
    </style>

</head>
<body>
    <div class="container">
        <header>
            <nav>
                <ul>
                    <li><a href="index.html" style="color: cornflowerblue;">Home</a></li>
                    <li><a href="https://github.com/yaotingwangofficial/yaotingwangofficial.github.io/raw/main/yaoting_wang_cv.pdf">Curriculum Vitae</a></li>
                    <li><a href="project.html">Project & Publication</a></li>
                    <li><a href="gallery.html">Gallery</a></li>
                </ul>
            </nav>
            <br>
            <h1>Welcome to Yaoting Wang's Home Page!</h1>
        </header>
        
        <main>
            <!-- <section id="about">
                <h2>关于我</h2>
                <p>你好，我是 [你的名字]。我是一名 [你的职业]，热衷于 [你的兴趣]。我致力于 [你的目标或成就]。</p>
            </section> -->

            <h2>Yaoting WANG</h2>
            <div class="photo-and-contact">
                <img src="media/yaoting.jpg" alt="My Photo" width="18%">
                <div>
                    <p> Yaoting WANG | 王耀霆 </p>
                    <br>
                    <p> Email: yaoting.wang@outlook.com </p>
                    <br>
                    <a href="https://scholar.google.com/citations?user=apa4kr4AAAAJ" target="_blank" class="icon">
                        <img src="./media/google.png" width="15%">
                    </a>
                    
                    <a href="https://github.com/yaotingwangofficial" target="_blank" class="icon">
                        <img src="./media/github.png" width="15%">
                    </a>

                    <a href="https://www.zhihu.com/people/wang-yao-ting-29" target="_blank" class="icon">
                        <img src="./media/zhihu.png" width="15%">
                    </a>
                </div>
            </div>
            
            <div id="intro">
                <h2>Introduction</h2>
                <p> Hallo! I'm Yaoting Wang, and I'm currently a research intern at Tsinghua University. My primary interests lie in Multimodal LLM, Multimodal NLP, and Audio-Visual Learning.
                </p>
            </div>

            <div>
                <section id="edu">
                    <h2>Education</h2>
                    <ul>
                        <li> PhD: soon... </li>
                        <li> MSc: University of Edinburgh (2021~2022) </li>
                        <li> BSc: University of Limerick (2019~2021) </li>
                        <li> BEng: Shandong University of Science and Technology (2017~2021)</li>
                    </ul>
                </section>
                <section id="edu">
                    <h2>Professional</h2>
                    <ul style="list-style-type: none; padding-left: 0;">
                        <li>
                            <strong>Research Intern</strong> @ THU AIR <span style="color: gray;">(02/2025 - Present)</span><br>
                        </li>
                        <li>
                            <strong>Visiting Student</strong> @ KAUST Vision-CAIR <span style="color: gray;">(04/2024 - 02/2025)</span><br>
                            <span style="margin-left: 1em;">
                                Advised by 
                                <a href="https://scholar.google.com.sg/citations?user=5HM8wcgAAAAJ" target="_blank">Dr. Jian Ding</a> and 
                                <a href="https://scholar.google.com/citations?user=iRBUTOAAAAAJ" target="_blank">Prof. Mohamed Elhoseiny</a>.
                            </span>
                        </li>
                        <li>
                            <strong>Research Assistant</strong> @ RUC GSAI <span style="color: gray;">(03/2023 - 03/2024)<span><br>
                            <span style="margin-left: 1em;">
                                Advised by 
                                <a href="https://scholar.google.com.hk/citations?user=F7bvTOEAAAAJ" target="_blank">Prof. Di Hu</a>.
                            </span>
                        </li>
                    </ul>
                </section>
            </div>
            
            <section id="news">
                <h2>News</h2>
                <div class="news_list">
                    <ul>
                        <li>
                            [18-03-2025] We release the first comprehensive survey on Multimodal Chain-of-Thought (MCoT) reasoning, along with the <a href='https://github.com/yaotingwangofficial/Awesome-MCoT' target="_blank">Awesome-MCoT</a> repository.
                        </li>
                        <li> 
                            [30-08-2024] <br> - Thanks <a href="https://www.qbitai.com/2024/08/186120.html" target="_blank">
                            QBitAI's report</a> for our work <a href="https://gewu-lab.github.io/Ref-AVS/" target="_blank">
                            Ref-AVS</a>, welcome to follow!
                        </li>
                        <li> 
                            [09-07-2024] <br> - Glad to have a 
                            <a href="https://www.bilibili.com/video/BV1dT421r7tm/" target="_blank">speech</a> 
                            on GAVS at <a href="https://valser.org/" target="_blank">Vision and Learning SEminar (VASLE)</a>!
                        </li>

                        <li>
                            [16-07-2024] <br> - We are excieted to release our new task 
                            <a href="https://gewu-lab.github.io/Ref-AVS/" target="_blank">Reference Audio-Visual Segmentation (Ref-AVS)</a>
                             and its benchmark dataset Ref-AVS Bench. How can we ask machines to locate objects of interest 
                             in the real-world with vision, audio, language... just like a human!
                        </li>

                        <li> 
                            [01-07-2024] <br> - <span style="color: red;">Three papers</span>
                            <a href="https://arxiv.org/abs/2407.10957" target="_blank">Ref-AVS</a>,
                            <a href="https://arxiv.org/abs/2407.10947" target="_blank">Segmentation-Preference</a> and
                            <a href="https://arxiv.org/abs/2407.11820" target="_blank">Stepping-Stones</a> 
                            have been accepted by <b>ECCV 2024</b>! 
                        </li>
                        <li>
                            [01-03-2024] <br> - Joined Vision-CAIR, KAUST!
                        </li>
                        <li> 
                            [09-12-2023] <br> - One paper 
                            <a href="https://arxiv.org/abs/2309.07929" target="_blank">GAVS</a> 
                            has been accepted by <b>AAAI 2024</b> main track and ICCV 2023 AV4D workshop! 
                        </li>
                        <li> 
                            [05-01-2023] <br> - Joined GeWu Lab, Gaoling School of Artificial Intelligence, Renmin University of China!
                        </li>
                    </ul>
                </div>
            </section>
            
        </main>
        
        <!-- <footer>
            <p>&copy; 2024 [你的名字]. 版权所有.</p>
        </footer> -->
    </div>
</body>
</html>
